\chapter{Numerical Dependencies in Databases and Data
Mining}\label{chap:numdep}

We now concentrate on introducing Numerical Dependencies (NDs) and
related theoretical and practical issues so that we are fully able to
appreciate later work utilising NDs in indefinite and temporal
relations.

\medskip

Initially, in section~\ref{sec:approx} we formalise the lattice theory
of NDs. We also show how ND values may be inaccurate for a given
relation and define {\em mean} NDs to combat such
problems. Section~\ref{sec:nd_chase} introduces the chase for NDs as a
precursor to the chase for NDs in indefinite relations in
Chapter~\ref{chap:consistency}. We discuss and extend the
axiomatisation of nds of \cite{gm85a} in section~\ref{sec:nd_axiom}
and present algorithms for data mining of NDs in
section~\ref{sec:nd_datamine}. Section~\ref{sec:nd_evolve} discusses,
briefly, an evolutionary algorithm for database design presented fully
in \cite{cl96}. Finally, sections~\ref{sec:nd_ar} discuss what an AR
for NDs might imply, given the discussion of
section~\ref{sec:nd_axiom} and we conclude with a general overview of
this chapter and its implications for data mining in section~\ref{sec:nd_disc}.


\section{Approximating FDs with NDs}\label{sec:approx}

We now define the lattice theory of NDs and then show how this may be
used to form a metric for approximating proximity to a given FD set. 

\subsection{The Lattice Theory of NDs}

Firstly, we present the lattice theory of NDs. We begin with
definition~\ref{def:more} which is then used to define the lattice of
NDs and definition~\ref{def:covered} which is used in our algorithm
for climbing the lattice.


\begin{definition}[More functional set of NDs]\label{def:more}
\begin{rm}
A set of NDs $N_1$ over R is {\em more functional} than a set of NDs 
$N_2$ over R, denoted by $N_2 \sqsubseteq N_1$, whenever
X $\to^{k_2}$ Y $\in N_2$ if and only if X $\to^{k_1}$ Y $\in N_1$ and 
$k_1 \le k_2$. 
\end{rm}
\end{definition}
\medskip

\begin{definition}[Covered By]\label{def:covered}
\begin{rm}
We say that $N_2$ is {\em covered by} $N_1$, denoted by $N_2$ $\cover$ $N_1$, where $N_1, N_2 \in {\cal L}_m$, 
if $N_1 \not= N_2$, $N_2 \sqsubseteq N_1$ and
$\forall N^\prime \in {\cal L}_m$ such that 
$N_2 \sqsubseteq N^\prime \sqsubseteq N_1$ we have $N^\prime = N_2$.
\end{rm}
\end{definition}

The set-theoretic relation, more functional than, 
is a partial order in the sets of NDs.
Assume that we are considering only sets of NDs over schema R which are more functional 
than a given set of NDs, N over R, each of the form X $\to^k$ Y, 
for some $k \ge 1$.
Then the family of sets of NDs that are more functional than N form a lattice
whose bottom element is N and whose top element is the set of FDs
induced by N, i.e. \{X $\to$ Y $\mid$ X $\to^k$ Y $\in$ N\}.
The {\em least upper bound}, $lub$, of $N_1$ and $N_2$ is the set of NDs
\{X $\to^{min(k_1, k_2)}$ Y $\mid$
X $\to^{k_1}$ Y $\in N_1$ and X $\to^{k_2}$ Y $\in N_2$\},
where $min(k_1, k_2)$ is the minimum of $k_1$ and $k_2$, and the 
{\em greatest lower bound}, $glb$, of $N_1$ and $N_2$ is defined similarly using maximum.
We call the lattice, whose top element is the set of FDs F over R
and whose bottom element is the set of NDs
\{X $\to^m$ Y $\mid$ X $\to$ Y $\in$ F\}, ${\cal L}_m$(F)
(or simply ${\cal L}_m$ if F is understood from context), with $m \ge 1$.


Therefore, we can {\em approximate} a set of FDs F by a set of NDs N
such that N $\sqsubseteq$ F. 
The {\em closer} N is to F in ${\cal L}_m$ the better the approximation is.
From now on we let ${\cal L}_m$ 
{\em be the lattice of NDs whose top element is} F
and assume that $\mid r \mid = m+1$, with $m \ge 1$. In
Figure~\ref{latt:1} we present a lattice for two NDs whose attributes
are not specified, over a relation with a maximum domain size of 4 in
the right hand side of each ND. The lattice significantly increases
with more NDs and larger domain sizes.


\begin{figure}
\centerline{\scalebox{0.55}{\includegraphics{../consistency/latt1.eps}}}
\caption{\label{latt:1}\scriptsize{Lattice of NDs for a relation of 2 FDs
(not shown) and maximum domain size of 4 for each dependency}}
\end{figure}



\medskip

\begin{definition}[Maximal set of NDs]
\begin{rm}
The {\em maximal} set of NDs of $r$ with respect to F,
denoted by $maximal(r$, F), is the maximal set N of NDs
in ${\cal L}_m$(F) (with respect to $\sqsubseteq$) such that $r \models$ N.
\end{rm}
\end{definition}
\medskip

Given $r$ and F, $maximal(r$, F) can be computed in polynomial time in the
sizes of $r$ and F by a straightforward hill climbing procedure
on ${\cal L}_m$(F). For each X $\to$ A $\in$ F this procedure finds 
the minimal $k$ such that $r \models$ X $\to^k$ A, starting 
from X $\to^m$ A which $r$ trivially satisfies since $\mid r \mid = m$. 


\begin{definition}[Improvement set of a set of NDs]
\begin{rm}
The {\em improvement set} of $r$ with respect to F, 
denoted by $\mu$($r$, F), is defined as 
\begin{displaymath}
\mu(r, {\rm F}) = \{{\rm X} \to^k {\rm A} \mid 
{\rm X} \to^k {\rm A} \in maximal(r, {\rm F}) \mbox{ and }  k > 1\}.
\end{displaymath}
\end{rm}
\end{definition}
\medskip


{\renewcommand{\baselinestretch}{1}
\begin{figure}[ht]
\begin{center}
\fbox{\begin{minipage}{16cm}
\begin{algorithm}[{\rm MU}($r$, {\rm F})]\label{alg:mu}
\begin{rm}
\begin{tabbing}
t1\=t2\=t3\=t4\=t5\= \kill \\
1.  \> \> {\bf begin} \\
2.  \> \> \> m := $\mid r \mid$; \\
3.  \> \> \> N := the bottom element of $\mathcal{L}_m$ (F); \\
4.  \> \> \> {\bf while} $\exists$ G such that N $\cover$ G {\em and} $r \models$ G  {\bf do} \\
5.  \> \> \> \> N := G; \\
6.  \> \> \>  {\bf end while} \\
7.  \> \> \> \> N := $ \{X \to^k Y \mid X \to^k Y \in N $ {\em and} $k > 1 \} $ \\
8. \> \> \> {\bf return} N;  \\
9. \> \> {\bf end.}
\end{tabbing}
\end{rm}
\end{algorithm}
\caption{\label{numdep:fig:mu} The improvement algorithm for NDs}
\end{minipage}}
\end{center}
\end{figure}
}


\subsection{Similarity Measures and Numerical Depdencies}


We introduce a measure for calculating the proximity of two ND sets using
their position within the lattice.  We show that this measure is a distance
function, satisfying reflexivity and symmetry, and is also a metric, 
satisfying the triangle inequality. Firstly, we begin by defining the best
approximation given by a set of NDs to their functional counterparts.
We define the {\em size} of a set of NDs N to be the number of attributes 
appearing in N including repetitions and define a {\em step}, either up
or down, to be exactly minus or plus one, respectively, to a single branch of
one ND within an ND set. 

\begin{definition}[The best approximation of a set of FDs]\label{def:best}
\begin{rm}
A set of NDs N over R is the {\em best approximation} of 
a set of FDs F over R with respect to a relation $r$ over R,
with $\mid r \mid = m+1$ (or simply the best approximation of F
if $r$ is understood from context), if $r \weak$ N 
and there does not exist a set of NDs, $N^\prime \in {\cal L}_m$
such that N $\cover$ $N^\prime$ and $r \weak N^\prime$.
\end{rm}
\end{definition}



\begin{proposition}[The number of NDs higher in the Lattice]
\begin{rm}
Given an ND set $N$ = \linebreak[4]   $\{ X_1 \to^{k_1} A_1, X_2 \to^{k_2} A_2,
 \ldots, X_n \to^{k_n} A_n \}$, the number of ND sets above this set in the
lattice is ($k_1 \cdot k_2 \cdot \ldots \cdot k_n$) - 1.
\end{rm}
\end{proposition}

{\em Proof.} An ND is higher in the lattice if within a 
set of NDs none of the $k_i$ branches have any values higher than
any of those in the set {\bf and} at least one of the NDs has some 
$k_j$ branch value
lower than one of those in the set. Each ND $X_i \to^{k_i} A_i$ within
the set can take $k_i$ values. We consider all permutations of these
values to get $k_1 \cdot k_2 \cdot \ldots \cdot k_n$ from which we 
must ensure that the ND set values of $N$ itself is not included to get
$k_1 \cdot k_2 \cdot \ldots \cdot k_n$ - 1. $\Box$

\smallskip

This provides us with the basis for a distance measure between an ND set
and its functional representation. However,  using this technique allows,
in some instances, ND sets which are the same number of steps below the
FD equivalent to have different values. This is due to ND sets containing
FDs or NDs which are close to being functional having less sets above them
in the lattice. To illustrate, if we have two ND sets $N_1,N_2$ each containing two NDs such that $N_1$ has dependencies with 4 and 2 as branches whilst $N_2$ has dependencies with 3 and 3 as branches then $N_1$ will have fewer ND sets
above it in the lattice though both are the same number of steps from their
functional equivalent, shown in Figure~\ref{latt:1}. We now introduce the 
metric we used in our simulations
and note that if we are interested in comparing ND sets with either more 
or less {\em near} FDs we can refer to the above measure 
whenever the metric provides the same distance.
\smallskip

In the following definition {\em distance} is defined as the number of
{\em steps} in the lattice. Definition~\ref{def:pfd} is used within
the simulations we conducted.

\begin{definition}[Proximity between two ND sets]
\begin{rm}
Given two sets of NDs $N_1$ and $N_2$ we define the metric as follows: 
\[
p(N_1,N_2) = \frac{\Sigma_{i = 1, 2} \mbox{ Distance from } N_i \mbox{ to }  lub \{ N_1, N_2 \}}
{\mbox{Maximum distance between any two ND sets to their $lub$ in the lattice}}
\]
\end{rm}
\end{definition}

\begin{definition}[Proximity to an FD set]\label{def:pfd}
\begin{rm}
Given a sets of NDs $N_1$ and a set of FDs $F$ which $N_1$ approximates
then the proximity between the two dependency sets is given by $p(N_1,F)$.
\end{rm}
\end{definition}

We define the bottom of the
lattice to be the set of NDs with each branching factor equivalent to
the domain size of the attribute on the right hand side of each
ND.

\smallskip
\begin{proposition}
\begin{rm}
The maximum distance between any two points in
the lattice to their $lub$ is always equivalent to the
distance from the bottom to the top of the lattice. 
\end{rm}
\end{proposition}

{\em Proof}. We prove this by induction on the NDs within the two ND sets.\newline 
\smallskip
\indent ({\em Basis}): We see that if $N_1$ and $N_2$ are empty then the
result is immediate.

\smallskip

({\em Induction}): We have two ND sets $N_1$ and $N_2$ which are distance
$d$ apart where $d \le m$ and $m$ is the maximum distance apart between
any two ND sets. We add an ND $X \to^{k_1} Y$
to $N_1$ and $X \to^{k_2} Y$ to $N_2$ which differ only
on their branching factor. Without loss of generality, if $k_1 < k_2$ then the
distance apart between $N_1$ and $N_2$ becomes $d + k_2 - k_1$. This 
remains less than or equal to the maximum distance apart which is $m + k'_2 -
k'_1$ where, without loss of generality, $k'_1 = 1$ (it is an FD) and
$k'_2$ is at the bottom of the lattice. $\Box$

\smallskip

The metric $p$ is a distance function given that the distance
between two NDs is zero only when they are equivalent and that 
$p(n_1,n_2) = p(n_2,n_1)$ always holds. It also satisfies the 
triangle inequality, whose proof we now outline. 

\begin{theorem}
\begin{rm}
Given three ND sets, $N_1$, $N_2$, and $N_3$, $p(N_1,N_2) + p(N_2,N_3)
\ge p(N_1,N_3)$.
\end{rm}
\end{theorem}
\smallskip

{\em Proof.} We show that if $N_1$, $N_2$ and $N_3$ are non-empty and the 
triangle
inequality holds then the addition of a new ND to each set which may
differ only on its branching factor will still satisfy the triangle inequality.
Assume we add three NDs $X \to^{k_i} A$ with $i = 1,2,3$ to $N_1,N_2$ and
$N_3$, respectively. We also assume, without loss of generality, that 
$k_1 < k_3$. We denote each ND set $N_i \cup \{ X \to^{k_i} A \}$ by $N'_i$
for $i = 1,2,3$. We perform induction on the NDs in each set.

\smallskip
({\em Basis}): If $N_1$ = $N_2$ = $N_3$ = $\emptyset$ then the 
result is immediate.

\smallskip
({\em Induction}):
 We assume that $k_2 \le k_1$, then $p(N'_1,N'_2)$ =
distance from $N_1$ to $lub(N_1,N_2)$ + distance from $N_2$ to $lub(N_1,N_2)$ +
$k_2 - k_1$. Similarly for $p(N'_2,N'_3)$ and $p(N'_1,N'_3)$ we have the
additional components, $k_3 - k_2$ and $k_3 - k_1$. Therefore, we have
$p(N'_1,N'_2) + p(N'_2,N'_3)$ = $p(N_1,N_2) + k_1 - k_2 + p(N_2,N_3) +
k_3 - k_2$ and $p(N'_1,N'_3)$ = $p(N_1,N_3) + k_3 - k_1$. We know that  
$p(N_1,N_2) + p(N_2,N_3) \ge p(N_1,N_3)$ holds and we see
 that  $k_1 - k_2 + k_3 - k_2 \ge k_3 - k_1$ holds if $k_1 \ge k_2$ which
is true, based on our initial assumption. We can similarly prove the 
triangle inequality for the case when $k_1 \le k_2$. $\Box$


\subsection{Partitioning a Relation for Mean NDs}\label{nd:subsec:mean}

In many data mining tools it is important that there exist measures
which accurately reflect the content of the database; this motivates
us to define mean ND set satisfaction for some situations.

\smallskip

In Chapter~\ref{chap:review} we presented definition~\ref{def:nd_part}
for partitioning of relation into blocks for an ND X $\to^k$ Y which
agree on X.
The satisfaction of an ND X $\to^k$ Y implies only that there exists
at least one partition ${\cal B}$ which contains at least $k$ tuples
with at most $k$ different Y-values.  There may however be numerous
other partitions on X which may have far less than $k$ different
Y-values and so the partition ${\cal B}$ dominates the relation and
presents an inaccurate representation of the proximity to FD set
satisfaction.  We therefore define the {\em mean numerical dependency}.


\begin{definition}[Mean Numerical dependency]
\begin{rm}
A {\em mean numerical dependency} over R (or simply an ND)
is a statement of the form X $\to^{\bar{k}}$ Y, where X, Y $\subseteq$ R and
$\bar{k} \ge 1$. We refer to $\bar{k}$ as the {\em mean branching factor}.
\end{rm}
\end{definition}
 
\begin{definition}[Satisfaction of a Mean ND]\label{def:sat-mean_nd}
\begin{rm}
Let $r$ be a relation over R.
An ND X $\to^{\bar{k}}$ Y is {\em satisfied} in $r$,
denoted by $r \models$ X $\to^{\bar{k}}$ Y, whenever
$r \models$ X $\to^k$ Y and $r$ is partitioned into blocks
$\{{\cal B}_1, {\cal B}_2, \ldots, {\cal B}_w\}$ with respect to X
$\to$ Y such that $\bar{k} = \frac{\sum_{i = 1}^{w} \mid
\pi_{\rm Y}({\cal B}_{i}) \mid }{w}$.
A set of averaged NDs $\bar{\rm N}$ is {\em satisfied} in $r$,
denoted by $r \models$ $\bar{\rm N}$, whenever
$\forall$ X $\to^{\bar{k}}$ Y $\in$ $\bar{\rm N}$, $r \models$ X $\to^{\bar{k}}$ Y.
\end{rm}
\end{definition}



\begin{example}
\begin{rm}
We assume that a relation $r$ over $AB$ satisfies the ND A $\to^{14}$ B
as its closest approximation to the FD A $\to$ B. However $r$ may only
contain three partitions  $\{{\cal B}_1, {\cal B}_2, {\cal B}_3\}\}$
with each partition satisfying then NDs A $\to^{14}$ B, A $\to^{1}$ B,
and A $\to^{1}$ B, respectively. We note the last two are satisfied
functionally. The mean ND set satisfaction is therefore A $\to^{5.67}$ B.
\end{rm}
\end{example}

In our work on indefinite information in relations we remark that we
are interested in exact ND set satisfaction only, given the nature of
the problem. In general data mining of NDs in standard and temporal
relations we may be interested in the mean ND set satisfaction
value. We note that if this value is vastly different from the exact
satisfaction value then it is likely that one or more partitions from
the relation {\em dominate} and remaining partitions will satisfy the
NDs more functionally.

\section{The Chase Procedure for NDs}\label{sec:nd_chase}
\index{Chase Procedure!for NDs}

We now show how CHASE($r$, F) can be generalised to CHASE($r$, N), where N is
a set of NDs over R, shown in Figure~\ref{alg:ndchase}.

{\renewcommand{\baselinestretch}{1}
\begin{figure}[ht]
\fbox{\begin{minipage}{16cm}
\begin{algorithm}[{\rm CHASE}($r$, {\rm N})]\label{alg:ndchase}
\begin{rm}
\begin{tabbing}
t1\=t2\=t3\=t4\=t5\=t6\=t7\= \kill \\
\ra.  \> \> {\bf begin} \\
\sa.  \> \> \> Result := $r$; \\
\sa.  \> \> \> Tmp := $\emptyset$; \\
\sa.  \> \> \> {\bf while} Tmp $\not=$ Result {\bf do} \\
\sa.  \> \> \> \> Tmp := Result; \\
\sa.  \> \> \> \> {\bf if} $\exists X \to^k Y \in$ N, $\exists t_1, t_2, \ldots, t_k, t_{k+1} \in$ Result such that  \\
    \> \> \> \> \> $t_1$[X] = $t_2$[X] = $\ldots$ = $t_k$[X] = $t_{k+1}$[X] \\
    \> \> \> \> \> but $t_1$[Y] $\not=$ $t_2$[Y] $\not= \ldots \not=$ $t_k$[Y] $\not=$ $t_{k+1}$[Y] {\bf then} \\ 
\sa. \> \> \> \> \> {\bf for each} $A \in$ Y$-$X {\bf do} \\
\sa. \> \> \> \> \> \> \> $t_i$[A],$t_j$[A], := max($t_i$[A],$t_j$[A])
    for two distinct values $i,j \in 1,\ldots,k+1$; \\
\sa. \> \> \> \> \> {\bf end for} \\
\sa.  \> \> \> \> {\bf end if} \\
\sa.  \> \> \> {\bf end while} \\
\sa. \> \> \> {\bf return} Result;  \\
\sa. \> \> {\bf end.}
\end{tabbing}
\end{rm}
\end{algorithm}
\caption{\label{numdep:fig:nd_chase} The chase procedure for NDs}
\end{minipage}}
\end{figure}
}

We leave it to the reader to verify that when $k = 1$, i.e. X $\to^k$ Y is an
FD, then CHASE($r$, N) reduces to CHASE($r$, F).


\begin{lemma}\label{th:algterm}
\begin{rm}
Algorithm~\ref{alg:ndchase} terminates.
\end{rm}
\end{lemma}

{\em Proof.} No new values are introduced into the algorithm at any
time and therefore the algorithm must halt after executing the while
loop a finite number of times. $\Box$

\begin{theorem}\label{th:chase}
\begin{rm}
Given a set of NDs, N, then $\forall n \in N$, CHASE($r$,N) $\models n$.
\end{rm}
\end{theorem}

{\em Proof.} Direct from the definitions of the algorithm and of ND
satisfaction. $\Box$


\begin{itemize}
\item The axiom system given in \cite{gm85a} is shown to be sound and
complete in the special cases of either $|R| \le 3$ or the number of
NDs with $k > 1$ is at most one.
\item \cite{gm85b} shows that there is no finite sound and complete
axiomatisation for NDs. 
\end{itemize}






\section{ND Axiomatisation}\label{sec:nd_axiom}


\subsection{Inferences for Numerical Dependencies}

Numerical dependencies allow a more general dependency relation than functional
dependencies.  \cite{gm85a,gm85b} introduce numerical dependencies with regard
to obtaining normal forms which avoid or minimise redundancy, from a database with k-dependency constraints. Grant and Minker also provide an axiomatization for numerical dependencies which is a generalisation of the Armstrong axioms for
functional dependencies. \cite{gm85a} presents a set of sound inference rules for NDs. \cite{gm85b}
shows that there does not exist a {\em finite} set of sound and
complete inference rules for Numerical Dependencies.
These axioms are shown to be complete for relations with have, at most,
3 attributes.  It is shown that any relation with more than 3 attributes is
not complete. We show in chapter~\ref{subsec:nd_ch_inf} how the chase may
be used as an inference procedure given that \cite{gm85a} shows that
there does not exist a finite axiomatisation for NDs.
If the relation utilises only functional dependencies then the axioms
contain the Armstrong rules as a subset.\\

\medskip
Rules 1-4 are as follows:
\begin{description}
\item[1] If $Y \subseteq X$ then infer $X \to Y$
\item[2] From $X \to^k Y$ infer $ZX \to^k ZY$
\item[3(a)] From $X \to^k Y$ and $Y \to^j Z$ infer $X \to^{k \cdot j} YZ$
\item[3(b)] From $X \to^k Y$ and $Y \to^j Z$ infer $X \to^{k \cdot j} Z$
\item[4] From $X \to^k Y$ infer $X \to^{k + 1} Y$
\end{description}

We now present another inference rule which generalises the rule
R5$_m$ of \cite{gm85b}, R6$_k$, which can be viewed as a generalised
transitivity rule for NDs wherein a bound on the number of attributes
required for inference is created based on the branching factor of the
NDs {\em and} the number of attributes on the left hand side of the FD
which determines attribute $Z$.

\smallskip
{\line
\begin{table}[ht]
\begin{tabular}{cl} \\
(R6$_{k,m}$): 	& from $\{ X \to^{k} Y_i \quad |  \quad 1 \le i \le \eta \} \cup 
		\{ Y_{i1}Y_{i2} \ldots Y_{im} \to Z   \quad |  \quad 1
		\le i1 < i2 < \ldots < im \le \eta \}$ \\ 
\rule{0cm}{5mm} & we can infer $X \to^{k} Z$ where $\eta =
		(m-1){k+1 \choose 2} + 1$\\
\end{tabular}
\end{table}}
\smallskip

\begin{theorem}\label{th:1}
\begin{rm}
Each rule R6$_{k}$ is sound and has minimal hypothesis.
\end{rm}
\end{theorem}

{\em Proof.} As for R5$_m$ in \cite{gm85b}, to be given. $\Box$

\medskip

We note that when $k$ = 1 then R6$_k$ reduces to transitivity of FDs,
and when $k$ = 2 then $\eta = 3m - 2$, the figure given in
\cite{gm85b}. 


\subsection{The Chase as an Inference Procedure}\label{subsec:nd_ch_inf}

We need to prove that the chase as an inference procedure for NDs can be shown
to be sound and complete. 

Given a set of NDs $N$ and an ND $\sigma$, we apply the chase as an
inference tool to discover if $N \models \sigma$. 
We create a relation $r_\sigma$ which for $\sigma = X \to^k
A$ has $k+1$ tuples with $k+1$ different values on attribute set $A$,
all values on X equivalent and all values in $R \backslash XA$ unique.

{\line
\begin{table}[ht]
\begin{center}
\begin{tabular}{c|c|c|c|c|c|} \cline{2-6}
 	& X 	& A 	& B$_1$ & $\ldots$ 	& B$_m$ \\ \cline{2-6}
$t_1$ 	& 1  	&  2  	& 1 	& $\ldots$ 	& 1 \\
$t_2$ 	& 1  	&  3  	& 2 	&  $\ldots$ 	& 2 \\
$\vdots$ & $\vdots$  &  $\vdots$  & $\vdots$ & $\ddots$ & $\vdots$ \\
$t_{k+1}$ & 1  &  $k+1$  & $k+1$ & $\ldots$ 	& $k+1$ \\ \cline{2-6}
\end{tabular}
\end{center}
\caption{\label{tbl:1.0} Relation to be chased by ND set N with
$\sigma = X \to^k A$, R $\backslash$ XA = $\{ B_1, \ldots, B_m \}$
and $m$ = $|$ R $\backslash$ XA $|$}
\end{table}}

Theorem~\ref{th:2} requires the notion of containment mapping.
For the definition we define a function $dom$ which returns the domain
of a relation.

\begin{definition}[Containment Mapping]\label{def:cm}
\begin{rm}
A containment mapping $\phi$ from $r_\sigma$ to a relation $r$ has
each value in $r_\sigma$ mapped by $\phi$ to a value in $dom$($r$).
This is extended to tuples over $R = A_1A_2 \ldots A_m$ as $\phi(t) =
\phi(t[A_1]),\phi(t[A_2]),\ldots, \phi(t[A_m])$ and extends to a
relation as $\phi(r) = \{ \phi(t) | t \in r \}$.
\end{rm}
\end{definition}

\begin{theorem}\label{th:2}
\begin{rm}
Given a set of Numerical Dependencies N and a ND $\sigma = X \to^k Y$,
N $\models \sigma$ iff $\neg\exists t^c_1[A] \not= t^c_2[A] \not= \ldots
\not= t^c_{k+1}[A]$ where $t^c_1,t^c_2,\ldots,t^c_{k+1} \in
r_\sigma^c$ and $r_\sigma^c$ = chase($r_\sigma$,N).
\end{rm}
\end{theorem}

{\em Proof.} (if) We assume that $Y \not\subseteq Z$. We let $r$ be a
relation over $R$ such that $r \models N$; we show that $r \models
\sigma$. Let $t_1, t_2, \ldots, t_{k+1} \in r$ such that $t_1[X] =
t_2[X] = \ldots = t_{k+1}[X]$. We claim that for some $i,
j \in \{1, 2, \ldots, {k+1}\}$ there exists $t_i[Y] = t_j[Y]$.

\smallskip
Let $\phi$ be a containment mapping from $r_\sigma$ to $r$ such that
$\phi(u_1) = t_1, \phi(u_2) = t_2, \ldots, \phi(u_{k+1}) = t_{k+1}$. We
shall prove that $\phi$ is additionally a containment mapping from
CHASE($r_\sigma$, N) to $r$ so that for some $i,j \in r_\sigma^c$,
$\phi(t_i^c[Y])$ = $\phi(t_j^c[Y])$ implies $t_i[Y] = t_j[Y]$.\\
\smallskip
We prove this by induction on the number of steps, $s$, required to compute
CHASE($r_\sigma$, N).\\

({\em Basis}):
If $s = 1$ then for some ND $W \to^k Z \in N$ two values are equated 
where $W \subseteq Z$ and $Y \subseteq Z$ and so therefore 
$t_i[Y] = t_j[Y]$ for some $i,j$.
\smallskip
 
({\em Induction}):
We assume that the result holds when $s$ steps of the chase procedure
are required. We now prove it to be true when $s+1$ chase steps are
required. We let the $s+1$ chase step be for an ND $W \to^k Z$ and
$w_i, w_j$ be two tuples in $r_\sigma$. The $s+1$ step either modifies
$w_i$ or $w_j$. For some subset $B \subseteq Z$ either $w_i[B]$ or
$w_j[B]$ is modified so that $w_i[B] = w_j[B] = t_i^c[Y] = t_j^c[Y]$.
Now, given $w_i[W] = w_j[W]$ then $\phi(w_i[W]) = \phi(w_j[W])$ by the
inductive hypothesis.  Therefore $\phi(t_i^c[Y]) = \phi(t_j^c[Y]) =
\phi(w_i[B]) = \phi(w_j[B]) = t_i[Y] = t_j[Y]$ holds given that $r
\models W \to^k Z$.\\

\smallskip

(only-if) If $\neg\exists t_i^c[Y] = t_j^c[Y]$ for some $i,j \in {1,2,
\ldots, k+1}$ then we can contruct a relation $r$ which satisfies all
$n \in N$ but but violates $\sigma$. \\

\smallskip
Alternatively, the chase for $r_\sigma$ can be shown to be isomorphic
to a relation $r$ which satisfies N by mapping each value in
$r_\sigma^c$ to a value in dom(r). $\Box$


\begin{theorem}\label{th:3}
\begin{rm}
The chase inference procedure for Numerical Dependencies is sound and complete.\end{rm}
\end{theorem}

{\em Proof.} Soundness of the chase as an inference procedure is
understood. Given a set of NDs N and an ND $\sigma = X \to^k A$ we
need to show completeness, ie. $N \models \sigma$ implies that $N
\vdash \sigma$. As for the proof procedure given in \cite{ll97c} we show
that the (if) part of Theorem~\ref{th:2} implies that $N \vdash
\sigma$. We show this by induction on the number of chase steps $s$
required to compute CHASE($r_\sigma$,N).

\smallskip
({\em Basis}):
If s = 1, then the chase procedure is applied once for some ND $W
\to^{k_2} Y$. This chase step equates two values in A and so $A \in Y$
 and $W \subseteq X$ (W must agree
on at least two tuples). Additionally $k_2 = k$ given that only one
chase step is required. We can infer $X \to^k Y$ by composition and $X
\to^k A$ by decomposition implying that $N \vdash \sigma$.
 
\smallskip
 
({\em Induction}):
Assume the result holds when the result of CHASE($r_\sigma$,N) takes 
$s$ steps. We prove that the result holds when the number of steps
required in $s+1$. The last chase step will apply to some ND $W \to^v Z
\in N$. For some $B \in Z$, two values $t_i[B]$ and $t_j[B]$,
initially distinct, will be equated.

\smallskip
If $B \not= A$ then we can assume that $N \vdash \sigma$ by the
 inductive hypothesis. We therefore assume that two values in A are
 equated in the $s+1$ step. We now consider the three cases that
 attribute set $W$
 may be related to attribute set $X$: \\

\begin{description}
\item[$W \subseteq X$] implies that we can infer $X \to^v A$ by
 composition. If $v > k$ then $v = k+1$ and no values will need to be
 equated in A, and so $v = k$. We infer that $N \vdash X \to^v A$ by
 decomposing $W \to^k Z$ to $W \to^k A$ and then apply the composition
 rule to obtain $X \to^k A$
\item[$W \supset X$] to be given
\item[$W$ is incomparable with $X$] to be given
\end{description} 
 $\Box$




\section{Numerical Dependencies in Data Mining}\label{sec:nd_datamine}

\subsection{Dependency Mining Applications}\label{subsec:fd_jobs}

Perhaps one of the prime applications for dependency mining is as a database
design tool which the database designer can use in conjunction with a possible
instance of the data to be stored within the database.  Inference upon this 
example set will then provide the designer with vital information as to possible
unknown dependencies that be satisfied in the relation. The approach of 
Bell and Brockhausen \cite{bb95} in making inferences from the verified and
invalid data dependencies is aimed at {\em supporting} the database
designer. \\

Approximation of the dependency set, possibly approximated using numerical
dependencies, on a large database in existence may reveal unknown information
in the form of these dependencies which may hold in the database.  

\begin{example}
\begin{rm}
In a patient database within a hospital every patient visit is independently stored
within a {\em patient} details relation and a {\em disease/sympton/treatment} relation. 
Given a numerical dependency specified $DISEASE \to^{10} SYMPTON$ stating
that a disease can have at most 10 symptons, it may however be approximated
that $ADDRESS \: \: \: PATIENT \to^{6} SYMPTON$ showing that at most
6 of the sympton can occur at the same location for a patient.
\end{rm}
\end{example}


\subsection{Mining a relation for a set of NDs}

We consider only singleton right hand sides. For a relation $R$ with
$n$ attributes we have $n 2^{n-1}$ NDs returned by this algorithm.

\medskip

{\renewcommand{\baselinestretch}{1}
\begin{figure}[ht]
\fbox{
\begin{minipage}{16cm}
\begin{algorithm}[{\rm ND\_mine}($r$, $R$)]\label{alg:mine}
\begin{rm}
\begin{tabbing}
t1\=t2\=t3\=t4\=t5\=t6\=t7\=t8\=t9\= \kill \\
\ra.  \> \> {\bf begin} \\
\sa.  \> \> \> ND\_set := $\emptyset$;\\
\sa.  \> \> \> {\bf for each} $A \in R$ {\bf do}\\
\sa.  \> \> \> \> {\bf for each} $W \in \mathcal{P}$(R - A) {\bf do}\\
\sa.  \> \> \> \> \> $\{ W \to^k A \}$ =  MU($r$,$W \to A$); \\
\sa.  \> \> \> \> \> ND\_set := ND\_set $\cup$ $\{ W \to^{k} A \}$; \\
\sa. \> \> \>  \> {\bf end for}; \\
\sa. \> \> \>    {\bf end for}; \\
\sa. \> \> \> {\bf return} ND\_set;  \\
\sa. \> \> {\bf end.}
\end{tabbing}
\end{rm}
\end{algorithm}
\caption{\label{numdep:fig:nd_mine} The ND mining algorithm}
\end{minipage}}
\end{figure}
}

\begin{itemize}
\item The computational complexity of this algorithm is O($n^2 2^{n-1}
\mid r \mid log \mid r \mid$)
\item Drop supersets of attributes? Restrict to singleton left and right
hand sides - O($n^3\mid r \mid log \mid r \mid$)
\item When the lhs of any ND is $\emptyset$ then the ND corresponds
directly to the domain size
\item The scale of the mining can be cut down by using axioms provided
in \cite{gm85b,gm85a} when exact details are not required for
dependencies of the form $W \to^k A$ where $W = YV$ and we already
know $Y \to^{k_1} A$ and $V \to^{k_2} A$ which would give $k \le K_1$
or $k \le k_2$, depending on the larger partition.
\end{itemize}


\begin{figure}
\centerline{\scalebox{0.6}{\includegraphics{../../CPP/Time/data/nd_mine_bc.eps}}}
\caption{\label{graph:nd_mine1}\scriptsize{Results for mining Mean and
standard NDs with arity of the lhs of each ND restricted upon the
breast cancer dataset}}
\end{figure}

In figure~\ref{graph:nd_mine1} we see how increases in the arity of
the left side of the NDs increase the time required to mine for sets
of NDs in increasing relation sizes. We have applied this to the
breast cancer data set \cite{bkm98} used in many data mining research
papers. The data set has 11 attributes and 699 tuples. We increase
these tuples by adding identifiers to each tuple and copying (as
given in \cite{hkp98}). Obviously in a relation with 11 attributes
there is little point in examining the powerset of attributes as it is
highly likely that FD with, say 7 or more, attributes on the left hand
side will be satisfied functionally and will be meaningless. This
point is reiterated in \cite{sf93}.

\subsection{Mining a relation for a set of Mean NDs}
 

\section{Evolving Example Relations to Satisfy FDs}\label{sec:nd_evolve}
\index{Evolutionary Algorithm}

We now briefly present an example of applying NDs to approximate FDs
in an evolutionary hill-climbing algorithm for creating probabilistic
example relations for use within database design applications. This
work summarises \cite{cl98}, and, as indicated in
section~\ref{subsec:reldbdes}, is related to work in the
Design-By-Example project of \cite{mr86}.

\smallskip

Example relations satisfying a given set of integrity constraints such as 
functional dependencies (FDs) are important during the database design 
activity in order to guide the designer towards the specification of a correct 
set of constraints for the application in hand \cite{sm81}. 



\subsection{Motivation}

If the example relation shown to the database designer is too large then 
the designer will {\em not} be able to assimilate all the knowledge embedded in
that relation. Thus it would be useful to be able to generate random examples 
relations that satisfy F and whose maximal cardinality is specified by the 
database designer; in general, such an example may not be an Armstrong
relation.

\smallskip

Our algorithm is evolutionary in that it incorporates a stochastic approach
for altering a relation by a {\em mutation} operation.
 The algorithm proceeds as
follows; initially a relation is randomly generated following
the input of the designer and a given FD set. This relation is then
 mutated based on a probabilistic selection of an unsatisfied FD  from the
given set and an attribute which assists violation of this FD in the relation.
We use NDs as an
approximation of the unsatisfied FDs in the relation. 
The mutations steer the relation
towards a final state wherein all of the FDs in the specified set are
satisfied. It is a simple algorithm, and indeed a basic tenet 
of evolutionary programming is to create algorithms 
which do not constrain evolution too severely,
 much like organic evolution \cite{bs93}. All evolved relations are then mined 
using a quality function whose criterion is exact satisfaction of the given FD set. Example relations using NDs to approximate FDs together
with quality functions for assessing the discovered knowledge 
can be used as the basis for many dependency data mining algorithms \cite{kdd96}.

\smallskip

A deterministic approach used to generate an Armstrong relation
(Algorithm 14.2, \cite{Mann92}) has the severe drawback
 in that the same relation
is generated every time the algorithm runs.  Our probabilistic
approach is advantageous in that different example relations may
be generated from equivalent domain sizes and the tuple size may be
increased or decreased by the designer as desired. Moreover, as long as 
the number of tuples exceed the minimum size required for an Armstrong relation
 \cite{bdfs84,mr86} then one may
be returned.  Below this number and a deterministic approach fails
 whereas our evolutionary approach complies with the desires of the user
and returns a relation which, if selected from a batch or {\em 
population} of evolutions, is likely to be as high a quality
as possible given the domain and tuple restriction.  From the user's
point of view it may often be highly beneficial to examine a smaller
relation of a high quality, but less than one, as opposed to a larger Armstrong
relation (with a quality of one). To illustrate some
features of the evolutionary process we now present an example.

\begin{example}
\begin{rm}
For the schema $R$ containing attributes TEACHER, HOUR, and ROOM
we impose the FD TEACHER HOUR $\to$ ROOM, implying that no teacher
can be give a class in more than one room at the same time.
 We denote TEACHER, HOUR, and ROOM by $T, H$ and $R$,
 respectively, and show a deterministic
generated Armstrong relation (AR) in Table~\ref{tab:thr1}, and an evolved 
Armstrong relation in Table~\ref{tab:thr2} (with an equivalent domain and one
less tuple).
\end{rm}
\end{example}

{\line
\begin{table}[ht]
\begin{minipage}[b]{8cm}
\begin{center}
\begin{tabular}{|c|c|c|} \hline
 T & H & R \\ \hline
 0 & 0 & 0 \\
 1 & 0 & 0 \\
 1 & 1 & 0 \\
 2 & 1 & 1 \\
 2 & 2 & 2 \\  \hline
\end{tabular}
\end{center}
\caption{\label{tab:thr1}Deterministic AR}
\end{minipage}
\hfill
\begin{minipage}[b]{8cm}
\begin{center}
\begin{tabular}{|c|c|c|} \hline
 T & H & R \\ \hline
 2 & 1 & 3 \\
 2 & 3 & 3 \\
 3 & 1 & 3 \\
 3 & 3 & 1 \\  \hline
\end{tabular}
\end{center}
\caption{\label{tab:thr2} Evolved AR}
\end{minipage}
\end{table}

}

In order to test the viability of our approach we conducted simulations
over 72 carefully chosen sets of FDs. 
 Each FD set was evaluated with respect to the average length of the
 evolution process and the final quality of the relations
 produced in batches
containing 1,000 runs, a single run being the process of mutating
a randomly generated relation until the given FD set is satisfied.
This was performed for around 80 batches, varying over domain and tuple
size which were both held constant within a batch. The domain and tuple
sizes each ranged on average from about one 
half to double the number of tuples required for a deterministic
Armstrong relation (based upon the specified FD example set).  The simulations
emphasised the validity of this approach within database
design, showing that many varying relations can be efficiently
evolved for an FD set with numerous domain and tuple inputs.
 They also showed that it is extremely useful to know the quality of an
 example relation, and additionally that Armstrong
relations are often formed within a batch. 



\index{Randomised Algorithm}
\subsection{Mutating relations}
\label{sec:mutate}

Herein, we present an algorithm for {\em mutating} a relation.
Informally, given a relation which does not satisfy F, MUTATE($r$, F) 
randomly selects an ND, say X $\to^k$ A, in the improvement set of $r$ and 
stochastically modifies some of the tuple values in $r$.
We then define the syntactic property of non-interfering NDs 
and show that if the selected ND and another ND
Y $\to^g$ C in $maximal(r$, F) are non-interfering then, after the mutation, 
$r$ will still satisfy Y $\to^g$ C.
The non-interference property is important,
since if we evolve a relation to satisfy a set of FDs by iterating the
mutation operation, then the evolution process will be more efficient 
when the NDs in $maximal(r$, F) are non-interfering.

\medskip

The {\em mutation} of a relation over R with respect to a set of FDs over R 
denoted by MUTATE($r$, F), is defined as the relation resulting from invoking 
Algorithm~\ref{alg:mutate} presented below. In this algorithm we use LHS
 to denote the left hand side of an ND and RHS to denote the right hand
side.  This random selection of a side removes any bias which might otherwise
have been incurred if the selection of an attribute to mutate were
taken over the whole ND, given that the left hand side may be any
length less than or equal to $\mid R \mid$ but the right hand side is always singleton.


{\line
\begin{figure}[ht]
\begin{center}
\fbox{\begin{minipage}{16cm}
\begin{algorithm}[{\rm MUTATE}($r$, {\rm F})]\label{alg:mutate}
\begin{rm}
\begin{tabbing}
t1\=t2\=t3\=t4\=t5\= \kill \\
1. \> \>{\bf begin}\\
2. \>\> \> Result := $r$;\\
3. \>   \>\> Uniformly randomly select an ND $X \to^k A \in$ MU($r$,F);\\
4. \>   \>\> Uniformly randomly select a tuple $t \in r$;\\
5.  \> \>  \>{\bf if} $r[X,t[X]] \models  X \to^{k - 1} A $ {\bf then}\\
6.  \> \> \>\> {\bf return} $r$;\\
7.\> \> \>{\bf end if} \\
8.  \> \>\>Uniformly randomly select RHS or LHS of ND \\
9.  \> \>   \>{\bf if} LHS {\bf then}\\
10. \> \>  \>\> Uniformly randomly select an attribute $B \in X$\\
11. \> \>   \> \>Uniformly randomly select a value $v \in \mathcal{D} - \{t[B]\}$; \\
12. \> \> \> {\bf else} \% B = A\\
13. \> \> \> \> B := A;\\
14. \> \> \>\>  Uniformly randomly select a value $v \in \pi_A(r[X,t[X]]) - \{t[A]\}$; \\
15.  \> \> \>{\bf end if} \\
16. \> \> \>{\bf for each} $u \in r[X,t[X]]$ such that $u[XA] = t[XA]$  {\bf do}\\
17.\> \> \> \> $u[B]$ := $v$;\\
18. \> \> \>{\bf end for} \\
19. \> \> \> {\bf if} Result $ \models  X \to^k A $ {\bf then}\\
20.  \> \> \>\> {\bf return} Result;\\
21. \> \> \>{\bf else}\\
22. \> \> \>\> {\bf return} $r$;\\
23. \> \> \>{\bf end if}\\
24.  \>  \> {\bf end. }\\
\end{tabbing}
\end{rm}
\caption{\label{numdep:fig:mutate} The MUTATE procedure for
evolving relations}
\end{algorithm}
\end{minipage}}
\end{center}
\end{figure}
}
\medskip

We call the attribute chosen in line 8 of Algorithm~\ref{alg:mutate}
{\em the attribute chosen for mutation}.
We note that the selection process of NDs, tuples, attributes and values
in Algorithm~\ref{alg:mutate} is done uniformly randomly but 
we could introduce a {\em bias} by appropriately weighting 
elements in the selection process. For example, we may give a higher weighting
to the attribute A on the right-hand side of the ND,
at line 8 of the algorithm, so that the algorithm is more likely to 
execute line 13 than 10. As another example, we may give
higher weighting to values in $\cal D$ that are already used as values in
 some tuple of $r$, at line 10 of the algorithm, so as not to encourage
 too many tuples in $r$ to have disjoint values.

\smallskip

The following definition provides us with a measure of how useful a mutation 
is in the evolution of a relation to satisfy a set of FDs.

\begin{definition}[Useful, neutral and damaging mutations]
\begin{rm}
Let $s$ be the relation resulting from the mutation MUTATE($r$, F).
Then a mutation such as $s$ is said to be {\em useful}, {\em neutral} or 
{\em damaging}, respectively, for an ND Y $\to^g$ C,
if the number of blocks ${\cal B}_i$
in the partitioning of $s$ with respect to Y $\to^g$ C
such that ${\cal B}_i \not\models$ Y $\to^g$ C
is less than, equal to or greater than, respectively, 
the number of blocks ${\cal B}_i$ 
in the partitioning of $r$ with respect to Y $\to^g$ C
such that ${\cal B}_i \not\models$ Y $\to^g$ C
\end{rm}
\end{definition}
\medskip

It follows that MUTATE($r$, F) is always either neutral or useful for 
X $\to^{k-1}$ A, assuming that 
X $\to^k$ A is the ND chosen at line 3 of Algorithm~\ref{alg:mutate},
but may be damaging for some other ND Y $\to^g$ C.
In addition, consider a sequence of $q$ mutations, where
$0 \le i \le q$, $r = r_0$ and $r_{i+1}$ is the relation resulting from
the mutation MUTATE($r_i$, F).
Then, in order that the resulting relation satisfy X $\to^{k-1}$ A,
it is necessary that at least $m$ out of $q$ mutations are useful for 
X $\to^{k-1}$ A, where $m$ blocks in $r$ violate X $\to^{k-1}$ A.
On the other hand, it is possible that some of these mutations may 
be damaging for one or more NDs of the form Y $\to^g$ C.

\medskip

In the next theorem we exhibit sufficient and necessary conditions 
for a mutation MUTATE($r$, F) to be neutral 
for an ND Y $\to^g$ C $\in maximal(r$, F). If Y $\to^g$ C is the ND 
X $\to^k$ A which is chosen at line 3 of Algorithm~\ref{alg:mutate},
then the result is evident, since as noted above MUTATE($r$, F) 
cannot be damaging for the ND X $\to^{k-1}$ A.
On the other hand, we observe that MUTATE($r$, F) cannot be useful 
for Y $\to^g$ C, since $r \models$ Y $\to^g$ C.


\begin{definition}[Non-interfering NDs]
\begin{rm}
Two NDs X $\to^k$ A and Y $\to^g$ C are said to be {\em non-interfering} if 
either A = C and Y = X, or YC $\cap$ XA $= \emptyset$, 
or A $\not=$ C, X = \{C\} and YC = R.
\end{rm}
\end{definition}
\medskip

\begin{theorem}\label{theorem:neutral}
\begin{rm}
Assuming that X $\to^k$ A is the ND chosen at line 3 of 
Algorithm~\ref{alg:mutate}, then for all relations $r$ over R,
any mutation MUTATE($r$, F) is neutral for Y $\to^g$ C $\in maximal(r$, F),
if and only if X $\to^k$ A and Y $\to^g$ C are non-interfering NDs.
\end{rm}
\end{theorem}
{\em Proof.} 
Given in \cite{cl96}.$\Box$
\medskip


We call a set of NDs N over R such every pair of FDs in N is non-interfering a
{\em non-interfering} set of NDs.

\medskip

An attribute B in the left-hand side of an FD X $\to$ A is said to be 
{\em redundant} with respect to a set of FD F over R, if A $\in$ (X$-$B)${}^+$.
Assuming that no left-hand sides of FDs in F have redundant attributes,
it can be shown that when X $\to^k$ A is the ND chosen at line 3 of 
Algorithm~\ref{alg:mutate}, then the probability that any mutation MUTATE($r$, F)
is neutral for Y $\to^g$ C $\in maximal(r$, F), is at least 1 / $\mid$XA$\mid$.
Now consider the mutation, MUTATE($r$, F) assuming that 
B $\to^2$ A is the ND chosen at line 3 of Algorithm~\ref{alg:mutate},
where N = $maximal(r$, F) = \{A $\to$ C, B $\to^2$ A, D $\to$ B\}
and $r$ is shown in Table~\ref{tbl17}.
It can easily be verified, with a probability of one, that this mutation is 
damaging for one of D $\to$ B or A $\to$ C.
Thus, in this pathological case, no mutation of $r$ with respect to 
any ND in N can be neutral or useful with respect to all of the NDs in N
and thus, at times, it is necessary to accept mutations that are damaging 
to some of the NDs in N. 

{\line
\begin{table}[ht]
\begin{center}
\begin{tabular}{|c|c|c|c|} \hline
A & B & C & D \\ \hline
1 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 \\ \hline
\end{tabular}
\end{center}
\caption{\label{tbl17} A pathological example}
\end{table}
}
\medskip


\subsection{An Algorithm for Evolving Relations to satisfy FDs}
\label{sec:evolve}


Herein, we present our algorithm for evolving a relation $r$ to satisfy a set
of FDs F. The algorithm, ITERATE($r$, F) simply iterates 
the mutation operation on the current state of $r$ until the set of
FDs is satisfied. The number of iterations required is denoted by $q$.
We show that there always exists a finite number of states $q$
such that ITERATE($r$, F) satisfies F with a probability of one.
This leads us to model the evolution process as an 
{\em absorbing Markov chain} \cite{ks60} and to give a 
formula for the expected number of mutations necessary to
evolve a random relation.
The {\em iteration} of a relation, denoted by ITERATE($r$, F),
is defined as the result of invoking Algorithm~\ref{alg:iter},
presented below. The mutations are repeated until F is satisfied in $r$.
We say that ITERATE($r$, F) {\em evolves} the relation it returns
in $q$ {\em steps}, and that $r_2$ {\em evolves} from $r_1$
if ITERATE($r_1$, F) evolves $r_2$.

{\line
\begin{figure}[ht]
\begin{center}
\fbox{\begin{minipage}{16cm}
\begin{algorithm}[ITERATE($r$,F)]\label{alg:iter}
\begin{rm}
\begin{tabbing}
t1\=t2\=t3\=t4\=t5\= \kill \\
1.  \> \> \> {\bf begin}\\
2.  \> \> \> \>  Result  :=  $r$; \\
3. \> \> \> \>  $q$  :=  0; \\
3. \> \> \> \>   {\bf while} Result $\not\models$ F {\bf do}  \\
4. \> \> \> \>   \> Result := MUTATE(Result,F); \\
5. \> \> \> \>   \> $q$ := $q + 1$; \\
6.\> \> \> \>   {\bf end while};\\
7. \> \> \> \>   {\bf return} Result, $q$; \\
8.  \> \> \> {\bf end.} \\
\end{tabbing}
\end{rm}
\end{algorithm}
\caption{\label{numdep:fig:iterate} The ITERATE procedure for
evolving relations}
\end{minipage}}
\end{center}
\end{figure}
}
\medskip

The next theorem is fundamental for the Markov chain analysis of 
the evolution of a relation to satisfy a set of FDs.

\begin{theorem}\label{theorem:iter}
\begin{rm}
Let N be any set of NDs over R such that N $\sqsubseteq$ F.
Then there exists a finite $q$ such that ITERATE($r$, F) 
evolves a relation that satisfies N, with positive probability.
\end{rm}
\end{theorem}
{\em Proof.} 
Given in \cite{cl96}. $\Box$
\medskip

We model the evolution of a relation as an absorbing Markov chain in \cite{cl96}.


\subsection{Simulation Results}
\index{Active Domain Size}
\index{Simulations}


We now detail the simulations conducted to examine the
viability of evolving example relations from an initial
random relation. The designer can select and vary the maximum tuple size
of an example relation as well as the maximum domain size of the
attributes for any 
FD set. With such a large possible input space it was necessary to
 perform extensive simulations to test
the efficiency of generating random examples as well as assessing
the quality of the examples in terms of proximity to an Armstrong 
relation. We stress that the
variation for generating relations is completely up to the
designer; for an FD set he may wish to view example relations
of any tuple or domain size. Analysing the differences between example relations
 may highlight the need for perhaps
an additional dependency in the specified set, particularly if it
is known exactly how close to an Armstrong relation each example is. 
This section
also investigates FD sets whose examples tend to have a low
quality, as well as presenting a number of examples, some of 
which have had real-world
semantics imposed so that the reader can form a clearer picture
of how our approach may be incorporated into an application.

\medskip

{\line
\begin{table}[ht]
\begin{center}
\begin{tabular}{|l||l|}
\hline
{\bf Number of FD sets}  & 72 \\ \hline
{\bf Single FD simulations} & 1 batch for each domain/tuple combination\\ \hline
{\bf Batch Range} & 1,000 runs in each \\ \hline
{\bf Domain Range} & $G/2 - 2G$ where $ G = \mid GEN(F) \mid$  \\ \hline
{\bf Tuple Range} & $G/2 - 3G$  where $ G = \mid GEN(F) \mid$  \\ \hline 
\end{tabular}
\end{center}
\caption{\label{table:5.01} Simulation details }
\end{table}
}


We now describe the experiment in detail. In Table~\ref{table:5.01} a
run refers to the process of  of mutating
a randomly generated relation until the given FD set is satisfied.
 Each FD set was evaluated with respect to the average length of the
 evolution process and the average and maximal quality of the relations
 produced in batches of 1,000 runs.
This was performed for many batches, varying over domain and tuple
sizes, both held constant within a batch. As Table~\ref{table:5.01} 
shows, the batches ranged from having a domain and tuple size
of {\em around} half the cardinality of $GEN(F)$ to
a domain and tuple size of double the cardinality of $GEN(F)$.
  For a typical set of FDs this implies that around 80 batches
 were carried out. The spread of batches provided all of the useful
information; outside this range and smaller relations satisfy the
FD set trivially whilst results for larger relations can be gathered
from extrapolating within our range. This spread also covered the
algorithms behaviour relative to a deterministic generation
of an Armstrong relation which always produces a relation with
a tuple size of $\mid GEN(F) \mid + 1$.
For experimental purposes many sets
were extracted from numerous database papers and texts to further
examine the algorithm's behaviour.

\medskip


The program, a direct implementation of Algorithm~\ref{alg:iter},
 was written in C++ with the embedded CORAL deductive
database C++ interface to manipulate the relations (see
\cite{rss92} for an introduction to CORAL).  For the random
number generation a linear congruent procedure was used taken
from the algorithm provided by Park and Miller \cite{pm88} which
avoids cycles by incorporating multiplier and modulus
having 534 million full period generators. C++ with
embedded CORAL was also used for assessing the quality of
the relations after evolution, the knowledge discovery component
of our system.

\medskip

We now compare the absorption rates (number of states to
evolution) of two typical sets of FDs, 
interfering and non-interfering BCNF. Figure~\ref{graph:16_82}
shows the average number of evolutions to FD satisfaction over 1000 runs for
two BCNF FD sets, $F_1$ = $\{ A \to BC, BC \to A \}$ (non-interfering)
over $ABC$ and 
$F_2$ = $\{ A \to BCD, B \to A, C \to A \}$ (interfering) over $ABCD$.
 (We note that $X \to Y$ is an abbreviation for $X \to A_1, \ldots, X \to A_m$,
where $Y = \{A_1, \ldots, A_m \}$).
  All of the
FD sets used here were comparable in size and complexity given that the larger
the FD set the higher, on average, number of states to evolution required.
$F_2$ has an average number of states which increases
rapidly as the number of tuples is increased. This is due to the
interfering nature of the sets. To describe a possible mutation
for set $F_2$ a uniform random selection may choose to mutate violating
attribute $B$ for the FD $A \to BCD$. This however could be damaging for
the FD $B \to A$ and so our algorithm rejects this mutation. As we
can see from Figure~\ref{graph:16_82} it is the rejection of such
possible mutations that causes the increase in the number of states
to absorption. $F_1$ is non-interfering so that
any mutations will only ever be neutral for the other FDs in the set 
(Theorem~\ref{theorem:neutral}) creating fewer rejected mutations
and faster absorption rates. The absorption rate of an FD set also
rises the more interfering FD pairs there are within
the set.
Figure~\ref{graph:16_82}  also highlights another
aspect our evolutionary process, namely that we can generally not determine
 a difference in the 
absorption rates between BCNF and non-BCNF FD sets of a comparable
size. \\


\begin{figure}
\centerline{\scalebox{0.7}{\includegraphics{../Evolve_papers/sets50_82.eps}}}
\caption{\label{graph:16_82}\scriptsize{Average states to absorption for sets $F_1$ and $F_2$, Domain sizes: 3, 6 }}
\end{figure}

Most evolved relations for FD sets achieved a quality of 1 once the
tuple size was above $\mid GEN(F) \mid + 1$, detailed in \cite{cl96}.
 This is because the probability
of evolving an Armstrong relation is evidently lower when the tuple size
 is below
$\mid GEN(F) \mid + 1$. With a larger domain the chance of an example
relation being Armstrong is significantly lower, especially when the
domain and tuple sizes are comparable, often leading to a trivial
satisfaction of the FDs as well as FDs outside the specified set.
For an empty FD set over $R$ any random relation with schema $R$ satisfies this set;
in terms of quality every possible FD would need to be violated for such
a relation to be Armstrong.
 With a null FD set $\mid GEN(F) \mid = \mid R \mid$ and so
anything larger than a binary domain is unlikely to ever be an Armstrong
relation given the possible spread of all random relations. A measure
of the pathology of an FD set F can be
 provided by the ratio
of the determinations in F to the number of all possible determinations
which can occur over the schema $R$.
Given an attribute set and an FD set
which explicitly specifies all possible non-trivial FDs which
can hold amongst the attributes except for one FD then it is highly likely that
many relations will be evolved which in addition violate this FD. Thus within
such a batch it is likely that many example evolutions will be Armstrong
relations. The
pathology of FD sets has been examined and those which do not 
contain determinations between many of the attributes are
likely to have a lower proximity to an Armstrong relation; this does
not generally detract from their usefulness as example relations.

\medskip

The evolutionary procedure is now highlighted with a real world 
example, with the intention of providing the reader with an 
appreciation of the utility of varied example relations. It
can be said that a greater understanding of the semantics of 
an FD set
is reached by repeated examinations of different instances of the
example relations; this is one motivating factor behind our probabilistic approach.

\begin{example}
\begin{rm}
We use the following non-BCNF FD set F =
$\{ Name \to Phone$ $Flat No.$, $Flat No. \to Name$, $Postcode \to City \}$.
Again we present a deterministic Armstrong relation for this set of 
dependencies together with three different evolved Armstrong relations
of varying tuple and domain size. Table~\ref{table:5.31} shows
an Armstrong relation evolved using a deterministic process and an evolved
Armstrong relation is shown in Table~\ref{table:5.32}. A quick
inspection of these two relations shows that the differences in
Armstrong relations with the same domain and tuple sizes tend to be
superficial, yet the stochastic nature of the generation of relations leads
 a more well-rounded view of the data. Tables~\ref{table:5.33}
and~\ref{table:5.34} contain two other Armstrong relations, the former 
extending the domain size of the deterministic Armstrong relation slightly and the latter
presenting an Armstrong relation with a domain size 8 and 9 tuples. In
this instance a larger relation highlights both the satisfied
and violated dependencies thoroughly. 
\end{rm}
\end{example}

{\line
\begin{table}[ht]
\begin{center}
\begin{tabular}{|c|c|c|c|c|} \hline 
{ \bf Name} & { \bf Phone} & {\bf Flat no. }  & { \bf Postcode}  & {\bf City} \\ \hline
Dave & 1246 & 19 & NW1 & London  \\
Dave & 1246 & 19 & YO2 & York \\
Dan & 3748 & 7 & YO2 & York \\
Dan & 3748 & 7 & YO1 & York \\
Charles & 3748 & 11 & YO1 & York \\ \hline
\end{tabular}
\end{center}
\caption{\label{table:5.31} Mannila's Armstrong relation for set 29 }
\end{table}
}

{\line
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|} \hline 
{ \bf Name} & { \bf Phone} & {\bf Flat no. }  & { \bf Postcode}  & {\bf City} \\ \hline
Dave & 1246 & 19 & NW1 & London  \\
Dave & 1246 & 19 & YO2 & York \\
Dan & 3748 & 7 & NW1 & London \\
Dan & 3748 & 7 & W14 & London \\
Charles & 1246 & 11 & YO2 & York \\ \hline
\end{tabular}
\end{center}
\caption{\label{table:5.32} An evolved AR with the same domain size }
\end{table}
}

{\line
\begin{table}[ht]
\begin{center}
\begin{tabular}{|c|c|c|c|c|} \hline 
{ \bf Name} & { \bf Phone} & {\bf Flat no. }  & { \bf Postcode}  & {\bf City} \\ \hline
Dave & 1246 & 19 & YO1 & York  \\
Dave & 1246 & 19 & NW1 & London \\
Dan & 1246 & 7 & W14 & London \\
Dan & 1246 & 7 & NW1 & London \\
Charles & 3748 & 11 & YO2 & York \\ \hline
\end{tabular}
\end{center}
\caption{\label{table:5.33} An AR with a larger domain size }
\end{table}
}

{\line
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|} \hline 
{ \bf Name} & { \bf Phone} & {\bf Flat no. }  & { \bf Postcode}  & {\bf City} \\ \hline
Dave & 1246 & 19 & NW1 & London  \\
Dave & 1246 & 19 & W14 & London \\
Dave & 1246 & 19 & YO3 & York \\
Dan & 1246 & 7 & BS8 & Bristol \\
Dan & 1246 & 7 & BA1 & Bath \\
Dan & 1246 & 7 & BA2 & Bath \\
Charles & 1246 & 11 & YO2 & York \\
Matt & 8881 & 84 & BA8 & Bath \\
Fred & 2383 & 24 & YO3 & York \\ \hline
\end{tabular}
\end{center}
\caption{\label{table:5.34} An evolved AR with 9 tuples }
\end{table}
}

\medskip

We now move on to examine the pathological sets, these being
the sets for which an Armstrong relation was only rarely,
or in some cases never, achieved. Pathological sets are
defined as sets which contain attributes in the schema with none or
few
determinations specified in the FD set; they are more likely,
though not necessarily, to have a large $\mid GEN(F) \mid$. One
such set which is unlikely to have a high quality is the null
set of FDs. An example of sets which have an exponential $\mid GEN(F) \mid$
in the size of the FD set are discussed in \cite{bdfs84}.
These sets possess the following pattern: $\{ A_1A_2 \to B, A_3A_4 \to B, \ldots , A_{2m-1}A_{2m} \to B \}$ which has  $\mid GEN(F) \mid = 2^m$ and for
our simulations resulted in examples with a low proximity to Armstrong.
To exemplify why, any possible relationship between $A_i$ and $A_j, i
\not= j$ is not discouraged by our algorithm. Larger attribute combination sets are also
unlikely to be violated for anything above a binary domain, such as
$A_5A_3 \to A_1A_2A_6$, where there is little chance of repititions
within either the left or right hand sides.
As we have noted
our procedure only evolves relations by mutating attributes of FDs which are
inside the specified set and not by forcing violations of FDs which
hold outside of the specified set. This is discussed more fully in \cite{cl96}.

\medskip


To conclude, the results have shown that example relations which satisfy sets
of FDs can be efficiently evolved. The many different
relations which can be studied for the same FD set also provide a
more well-rounded view of the data in the designer's mind.
Batches containing
many evolutions can be run and a database designer would then be able
to view many relations, including those that are Armstrong if the domain
and tuple sizes satisfy the size bounds and an Armstrong relation
was actually evolved.  If they do not, either domain
or tuple size being too low, then the designer can view an
approximation to an Armstrong which a batch has provided. In
non-pathological cases it is likely that
this will be the best, or close to the best, approximation to Armstrong
which exists.



\section{Armstrong Relations for NDs}\label{sec:nd_ar}


\section{Discussion}\label{sec:nd_disc}



For design purposes the evolution of example relations has been shown to
be a useful tool.  A good database
design tool is based on ease of use for the designer and example relations
are a step in this direction.  To  study the applicability of a set of FDs
 the user can limit the number of tuples in
a relation as well as the domain size. The simulations have shown that 
informative example relations can be evolved by our process.  The 
average number of states to evolution is dependent on both
the nature (non-interfering or interfering) and size of the FD set and the size of the relation.
Example relations containing attributes independent of each other
are less likely to be evolved into Armstrong relations. For 63 out
of our 72 sets of FDs tested an Armstrong relation was evolved
for some domain/tuple combination; this is an important side-effect
of our approach and may form the basis for further research.\\

This work
will be of use to the database designer as an auxiliary tool to complement
the other stages of the design process.  From a schema the designer is now
 able to evolve many varied example relations.\\

